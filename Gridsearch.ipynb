{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a75737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7317882, 13)\n",
      "read_comment 0.03501586934580252\n",
      "like 0.02580487086290815\n",
      "click_avatar 0.007533327266004016\n",
      "forward 0.0038211876059220415\n",
      "favorite 0.0013424649372591687\n",
      "comment 0.00040462527272235326\n",
      "follow 0.0007211102884687126\n",
      "(421985, 4)\n",
      "   userid  feedid  date_  device  read_comment  comment  like   play    stay  \\\n",
      "0       8   71474      1       1           0.0      0.0   1.0  500.0  5366.0   \n",
      "1       8   73916      1       1           0.0      0.0   0.0  250.0  1533.0   \n",
      "2       8   50282      1       1           0.0      0.0   0.0  750.0  1302.0   \n",
      "\n",
      "   click_avatar  forward  follow  favorite  \n",
      "0           0.0      0.0     0.0       0.0  \n",
      "1           0.0      0.0     0.0       0.0  \n",
      "2           0.0      0.0     0.0       0.0  \n",
      "(106444, 512)\n",
      "(106444, 64)\n",
      "100%|████████████████████████████████████████| 24/24 [1:41:24<00:00, 253.51s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:03<00:00,  4.13it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:34<00:00,  2.91s/it]\n",
      "100%|███████████████████████████████████████████| 11/11 [00:13<00:00,  1.20s/it]\n",
      " 23%|████████▊                              | 156/688 [15:40<1:25:55,  9.69s/it]"
     ]
    }
   ],
   "source": [
    "!python treebase.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a06e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7317882, 13)\n",
      "read_comment 0.03501586934580252\n",
      "like 0.02580487086290815\n",
      "click_avatar 0.007533327266004016\n",
      "forward 0.0038211876059220415\n",
      "favorite 0.0013424649372591687\n",
      "comment 0.00040462527272235326\n",
      "follow 0.0007211102884687126\n",
      "(419646, 4)\n",
      "   userid  feedid  date_  device  read_comment  comment  like   play    stay  \\\n",
      "0       8   71474      1       1           0.0      0.0   1.0  500.0  5366.0   \n",
      "1       8   73916      1       1           0.0      0.0   0.0  250.0  1533.0   \n",
      "2       8   50282      1       1           0.0      0.0   0.0  750.0  1302.0   \n",
      "\n",
      "   click_avatar  forward  follow  favorite  \n",
      "0           0.0      0.0     0.0       0.0  \n",
      "1           0.0      0.0     0.0       0.0  \n",
      "2           0.0      0.0     0.0       0.0  \n",
      "(106444, 512)\n",
      "(106444, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [54:16<00:00, 203.53s/it]\n",
      "100%|██████████| 9/9 [00:01<00:00,  4.73it/s]\n",
      "100%|██████████| 8/8 [00:22<00:00,  2.80s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n",
      "100%|██████████| 488/488 [23:02<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29759.84 Mb, 9260.75 Mb (68.88 %)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def reduce_mem(df, cols):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in tqdm(cols):\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "## 从官方baseline里面抽出来的评测函数\n",
    "\n",
    "def uAUC(labels, preds, user_id_list):\n",
    "    \"\"\"Calculate user AUC\"\"\"\n",
    "    user_pred = defaultdict(lambda: [])\n",
    "    user_truth = defaultdict(lambda: [])\n",
    "    for idx, truth in enumerate(labels):\n",
    "        user_id = user_id_list[idx]\n",
    "        pred = preds[idx]\n",
    "        truth = labels[idx]\n",
    "        user_pred[user_id].append(pred)\n",
    "        user_truth[user_id].append(truth)\n",
    "\n",
    "    user_flag = defaultdict(lambda: False)\n",
    "    for user_id in set(user_id_list):\n",
    "        truths = user_truth[user_id]\n",
    "        flag = False\n",
    "        # 若全是正样本或全是负样本，则flag为False\n",
    "        for i in range(len(truths) - 1):\n",
    "            if truths[i] != truths[i + 1]:\n",
    "                flag = True\n",
    "                break\n",
    "        user_flag[user_id] = flag\n",
    "\n",
    "    total_auc = 0.0\n",
    "    size = 0.0\n",
    "    for user_id in user_flag:\n",
    "        if user_flag[user_id]:\n",
    "            auc = roc_auc_score(np.asarray(user_truth[user_id]), np.asarray(user_pred[user_id]))\n",
    "            total_auc += auc\n",
    "            size += 1.0\n",
    "    user_auc = float(total_auc)/size\n",
    "    return user_auc\n",
    "\n",
    "y_list = ['read_comment', 'like', 'click_avatar', 'forward', 'favorite', 'comment', 'follow']\n",
    "max_day = 15\n",
    "\n",
    "## 读取训练集\n",
    "train = pd.read_csv('data/wechat_algo_data1/user_action.csv')\n",
    "print(train.shape)\n",
    "for y in y_list:\n",
    "    print(y, train[y].mean())\n",
    "\n",
    "## 读取测试集\n",
    "test = pd.read_csv('data/wechat_algo_data1/test_b.csv')\n",
    "test['date_'] = max_day\n",
    "print(test.shape)\n",
    "\n",
    "## 合并处理\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "print(df.head(3))\n",
    "\n",
    "## 读取视频信息表\n",
    "feed_info = pd.read_csv('data/wechat_algo_data1/feed_info.csv')\n",
    "\n",
    "## 读取feed embedding\n",
    "from sklearn.decomposition import PCA\n",
    "def feed_embedding_pca():\n",
    "    feed_embedding = pd.read_csv('data/wechat_algo_data1/feed_embeddings.csv')\n",
    "    feed = np.array(feed_embedding.feed_embedding.str.split().tolist())\n",
    "    print(feed.shape)\n",
    "    pca = PCA(n_components=64)\n",
    "    res = pca.fit_transform(feed)\n",
    "    print(res.shape)\n",
    "    df = pd.DataFrame(res)\n",
    "    df['feedid'] = feed_embedding['feedid']\n",
    "    return df\n",
    "feed_embedding = feed_embedding_pca()\n",
    "feed_embedding = feed_embedding.set_index('feedid')\n",
    "\n",
    "## 此份baseline只保留这三列\n",
    "feed_info = feed_info[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id', 'machine_tag_list', 'manual_tag_list', 'machine_keyword_list', 'manual_keyword_list'\n",
    "]]\n",
    "\n",
    "df = df.merge(feed_info, on='feedid', how='left')\n",
    "df = df.merge(feed_embedding, on='feedid', how='left')\n",
    "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
    "df['videoplayseconds'] *= 1000\n",
    "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
    "df['is_finish'] = (df['play'] >= df['videoplayseconds']).astype('int8')\n",
    "df['play_times'] = df['play'] / df['videoplayseconds']\n",
    "\n",
    "play_cols = ['is_finish', 'play_times', 'play', 'stay']\n",
    "\n",
    "# tag\n",
    "df['machine_tag_list'] = df['machine_tag_list'].apply(lambda x: str(x).split(\";\") if str(x)!='nan' else [])\n",
    "df['machine_tag_1'] = df['machine_tag_list'].apply(lambda x: int(x[0].split(\" \")[0]) if len(x)>0 else np.nan)\n",
    "df['machine_tag_2'] = df['machine_tag_list'].apply(lambda x: int(x[1].split(\" \")[0]) if len(x)>1 else np.nan)\n",
    "df['machine_tag_3'] = df['machine_tag_list'].apply(lambda x: int(x[2].split(\" \")[0]) if len(x)>2 else np.nan)\n",
    "df['machine_tag_4'] = df['machine_tag_list'].apply(lambda x: int(x[3].split(\" \")[0]) if len(x)>3 else np.nan)\n",
    "\n",
    "df['manual_tag_list'] = df['manual_tag_list'].apply(lambda x: str(x).split(\";\") if str(x)!='nan' else [])\n",
    "df['manual_tag_1'] = df['manual_tag_list'].apply(lambda x: int(x[0]) if len(x)>0 else np.nan)\n",
    "df['manual_tag_2'] = df['manual_tag_list'].apply(lambda x: int(x[1]) if len(x)>1 else np.nan)\n",
    "df['manual_tag_3'] = df['manual_tag_list'].apply(lambda x: int(x[2]) if len(x)>2 else np.nan)\n",
    "df['manual_tag_4'] = df['manual_tag_list'].apply(lambda x: int(x[3]) if len(x)>3 else np.nan)\n",
    "\n",
    "# keyword\n",
    "df['machine_keyword_list'] = df['machine_keyword_list'].apply(lambda x: str(x).split(\";\") if str(x)!='nan' else [])\n",
    "df['machine_keyword_1'] = df['machine_keyword_list'].apply(lambda x: int(x[0]) if len(x)>0 else np.nan)\n",
    "df['machine_keyword_2'] = df['machine_keyword_list'].apply(lambda x: int(x[1]) if len(x)>1 else np.nan)\n",
    "df['machine_keyword_3'] = df['machine_keyword_list'].apply(lambda x: int(x[2]) if len(x)>2 else np.nan)\n",
    "df['machine_keyword_4'] = df['machine_keyword_list'].apply(lambda x: int(x[3]) if len(x)>3 else np.nan)\n",
    "\n",
    "df['manual_keyword_list'] = df['manual_keyword_list'].apply(lambda x: str(x).split(\";\") if str(x)!='nan' else [])\n",
    "df['manual_keyword_1'] = df['manual_keyword_list'].apply(lambda x: int(x[0]) if len(x)>0 else np.nan)\n",
    "df['manual_keyword_2'] = df['manual_keyword_list'].apply(lambda x: int(x[1]) if len(x)>1 else np.nan)\n",
    "df['manual_keyword_3'] = df['manual_keyword_list'].apply(lambda x: int(x[2]) if len(x)>2 else np.nan)\n",
    "df['manual_keyword_4'] = df['manual_keyword_list'].apply(lambda x: int(x[3]) if len(x)>3 else np.nan)\n",
    "\n",
    "## 统计历史5天的曝光、转化、视频观看等情况（此处的转化率统计其实就是target encoding）\n",
    "n_day = 7\n",
    "for stat_cols in tqdm([['userid'], ['feedid'], ['authorid'], ['bgm_song_id'], ['bgm_singer_id'], ['manual_keyword_1'], ['machine_keyword_1'],['manual_tag_1'], ['machine_tag_1'],['userid', 'authorid'], ['userid', 'bgm_song_id'], ['userid', 'bgm_singer_id'], ['userid','manual_keyword_1'], ['userid', 'machine_keyword_1'], ['userid', 'manual_tag_1'], ['userid', 'machine_tag_1']]):\n",
    "    f = '_'.join(stat_cols)\n",
    "    stat_df = pd.DataFrame()\n",
    "    for target_day in range(2, max_day + 1):\n",
    "        left, right = max(target_day - n_day, 1), target_day - 1\n",
    "        tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
    "        tmp['date_'] = target_day\n",
    "        tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
    "        g = tmp.groupby(stat_cols)\n",
    "        tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
    "        feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
    "\n",
    "        for x in play_cols[1:]:\n",
    "            for stat in ['max', 'mean']:\n",
    "                tmp['{}_{}day_{}_{}'.format(f, n_day, x, stat)] = g[x].transform(stat)\n",
    "                feats.append('{}_{}day_{}_{}'.format(f, n_day, x, stat))\n",
    "\n",
    "        for y in y_list:\n",
    "            tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
    "            tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
    "            feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
    "\n",
    "        tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
    "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
    "        del g, tmp\n",
    "\n",
    "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
    "    del stat_df\n",
    "    gc.collect()\n",
    "\n",
    "## 全局信息统计，包括曝光、偏好等，略有穿越，但问题不大，可以上分，只要注意不要对userid-feedid做组合统计就行\n",
    "for f in tqdm(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'manual_keyword_1', 'machine_keyword_1', 'manual_tag_1', 'machine_tag_1']):\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "for f1, f2 in tqdm([['userid', 'feedid'], ['userid', 'authorid'], ['userid', 'bgm_song_id'], ['userid', 'bgm_singer_id'], ['userid', 'manual_keyword_1'],['userid', 'machine_keyword_1'],['userid', 'manual_tag_1'], ['userid', 'machine_tag_1']]):\n",
    "    df['{}_in_{}_nunique'.format(f1, f2)] = df.groupby(f2)[f1].transform('nunique')\n",
    "    df['{}_in_{}_nunique'.format(f2, f1)] = df.groupby(f1)[f2].transform('nunique')\n",
    "for f1, f2 in tqdm([['userid', 'authorid'], ['userid', 'bgm_song_id'], ['userid', 'bgm_singer_id'], ['userid', 'manual_keyword_1'], ['userid', 'machine_keyword_1'],['userid', 'manual_tag_1'], ['userid', 'machine_tag_1']]):\n",
    "    df['{}_{}_count'.format(f1, f2)] = df.groupby([f1, f2])['date_'].transform('count')\n",
    "    df['{}_in_{}_count_prop'.format(f1, f2)] = df['{}_{}_count'.format(f1, f2)] / (df[f2 + '_count'] + 1)\n",
    "    df['{}_in_{}_count_prop'.format(f2, f1)] = df['{}_{}_count'.format(f1, f2)] / (df[f1 + '_count'] + 1)\n",
    "df['videoplayseconds_in_userid_mean'] = df.groupby('userid')['videoplayseconds'].transform('mean')\n",
    "df['videoplayseconds_in_authorid_mean'] = df.groupby('authorid')['videoplayseconds'].transform('mean')\n",
    "df['feedid_in_authorid_nunique'] = df.groupby('authorid')['feedid'].transform('nunique')\n",
    "\n",
    "## 内存够用的不需要做这一步\n",
    "df = reduce_mem(df, [f for f in df.columns if f not in ['date_', 'machine_tag_list', 'manual_tag_list', 'machine_keyword_list', 'manual_keyword_list'] + play_cols + y_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b8f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7317882, 488)\n",
      "========= read_comment =========\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's auc: 0.92987\n",
      "[150]\tvalid_0's auc: 0.931834\n",
      "[200]\tvalid_0's auc: 0.932147\n",
      "[250]\tvalid_0's auc: 0.932336\n",
      "[300]\tvalid_0's auc: 0.932347\n",
      "[350]\tvalid_0's auc: 0.932402\n",
      "[400]\tvalid_0's auc: 0.932354\n",
      "[450]\tvalid_0's auc: 0.932364\n",
      "Early stopping, best iteration is:\n",
      "[367]\tvalid_0's auc: 0.932409\n",
      "0.6354736807695378\n",
      "runtime: 559.8797552585602\n",
      "\n",
      "========= like =========\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's auc: 0.841589\n",
      "[100]\tvalid_0's auc: 0.845295\n",
      "[150]\tvalid_0's auc: 0.846839\n",
      "[200]\tvalid_0's auc: 0.847663\n",
      "[250]\tvalid_0's auc: 0.847917\n",
      "[300]\tvalid_0's auc: 0.848087\n",
      "[350]\tvalid_0's auc: 0.848161\n",
      "[400]\tvalid_0's auc: 0.848241\n",
      "[450]\tvalid_0's auc: 0.84817\n",
      "[500]\tvalid_0's auc: 0.848087\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's auc: 0.848245\n",
      "0.6322108293666827\n",
      "runtime: 602.5686132907867\n",
      "\n",
      "========= click_avatar =========\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.856479\n",
      "[150]\tvalid_0's auc: 0.857297\n",
      "[200]\tvalid_0's auc: 0.857223\n",
      "[250]\tvalid_0's auc: 0.857247\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's auc: 0.857514\n",
      "0.7207928771446654\n",
      "runtime: 379.78973960876465\n",
      "\n",
      "========= forward =========\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's auc: 0.879876\n",
      "[100]\tvalid_0's auc: 0.882795\n",
      "[150]\tvalid_0's auc: 0.881428\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's auc: 0.88299\n",
      "0.7039665506699193\n",
      "runtime: 303.3812975883484\n",
      "\n",
      "[0.6354736807695378, 0.6322108293666827, 0.7207928771446654, 0.7039665506699193]\n",
      "0.6584079516137449\n",
      "========= read_comment =========\n",
      "Training until validation scores don't improve for 367 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0994735\n",
      "[200]\tvalid_0's binary_logloss: 0.0959859\n",
      "[300]\tvalid_0's binary_logloss: 0.0946356\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[367]\tvalid_0's binary_logloss: 0.0939618\n",
      "runtime: 619.2206282615662\n",
      "\n",
      "========= like =========\n",
      "Training until validation scores don't improve for 402 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0950167\n",
      "[200]\tvalid_0's binary_logloss: 0.0931906\n",
      "[300]\tvalid_0's binary_logloss: 0.0922751\n",
      "[400]\tvalid_0's binary_logloss: 0.0914792\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[402]\tvalid_0's binary_logloss: 0.0914567\n",
      "runtime: 734.6720397472382\n",
      "\n",
      "========= click_avatar =========\n",
      "Training until validation scores don't improve for 160 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0365945\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[160]\tvalid_0's binary_logloss: 0.0359039\n",
      "runtime: 355.2650330066681\n",
      "\n",
      "========= forward =========\n",
      "Training until validation scores don't improve for 90 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's binary_logloss: 0.01993\n",
      "runtime: 241.43430280685425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = df[~df['read_comment'].isna()].reset_index(drop=True)\n",
    "test = df[df['read_comment'].isna()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "cols = [f for f in df.columns if f not in ['date_', 'machine_tag_list', 'manual_tag_list', 'machine_keyword_list', 'manual_keyword_list'] + play_cols + y_list]\n",
    "print(train[cols].shape)\n",
    "\n",
    "trn_x = train[train['date_'] < 14].reset_index(drop=True)\n",
    "val_x = train[train['date_'] == 14].reset_index(drop=True)\n",
    "\n",
    "##################### 线下验证 #####################\n",
    "uauc_list = []\n",
    "r_list = []\n",
    "for y in y_list[:4]:\n",
    "    print('=========', y, '=========')\n",
    "    t = time.time()\n",
    "    clf = LGBMClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=5000,\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=2021,\n",
    "        metric='None'\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        trn_x[cols], trn_x[y],\n",
    "        eval_set=[(val_x[cols], val_x[y])],\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    val_x[y + '_score'] = clf.predict_proba(val_x[cols])[:, 1]\n",
    "    val_uauc = uAUC(val_x[y], val_x[y + '_score'], val_x['userid'])\n",
    "    uauc_list.append(val_uauc)\n",
    "    print(val_uauc)\n",
    "    r_list.append(clf.best_iteration_)\n",
    "    print('runtime: {}\\n'.format(time.time() - t))\n",
    "\n",
    "weighted_uauc = 0.4 * uauc_list[0] + 0.3 * uauc_list[1] + 0.2 * uauc_list[2] + 0.1 * uauc_list[3]\n",
    "print(uauc_list)\n",
    "print(weighted_uauc)\n",
    "\n",
    "##################### 全量训练 #####################\n",
    "r_dict = dict(zip(y_list[:4], r_list))\n",
    "for y in y_list[:4]:\n",
    "    print('=========', y, '=========')\n",
    "    t = time.time()\n",
    "    clf = LGBMClassifier(\n",
    "        learning_rate=0.02,\n",
    "        n_estimators=r_dict[y],\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=2021\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        train[cols], train[y],\n",
    "        eval_set=[(train[cols], train[y])],\n",
    "        early_stopping_rounds=r_dict[y],\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    test[y] = clf.predict_proba(test[cols])[:, 1]\n",
    "    print('runtime: {}\\n'.format(time.time() - t))\n",
    "test[['userid', 'feedid'] + y_list[:4]].to_csv(\n",
    "    'sub_%.6f_%.6f_%.6f_%.6f_%.6f.csv' % (weighted_uauc, uauc_list[0], uauc_list[1], uauc_list[2], uauc_list[3]),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
